% Contributors: Alexandre Lamy, Trung Vu
\section{The motivation for density-based clustering}

To motivate density-based clustering, we have to understand the
shortcomings of centroid-based clustering. The first is that it makes
some strong assumptions on what a cluster should look like. Consider the data set
in $\mathbb{R}^2$, shown in Figure \ref{fig:dataset}.

\begin{figure}
\begin{center}
\includegraphics[width=.4\linewidth]{chapter_2/files/raw.jpg}
\includegraphics[width=.29\linewidth]{chapter_2/files/intuition.jpg}
\includegraphics[width=.29\linewidth]{chapter_2/files/kmeans.jpg}
\end{center}
\caption{Our dataset in $\mathbb{R}^2$ (left), with the intuitive clustering (middle) and the $k$-means clustering (right).}
\label{fig:dataset}
\end{figure}

Our intuition suggests that each concentric circle should form its own cluster.
Thus the "ideal" clustering, according to our intuition, should look like the
second image in Figure \ref{fig:dataset}. However, running $k$-means on this dataset
produces the third image in Figure \ref{fig:dataset}.


Why is it the case that $k$-means fails to comply to our intuition in the example
above? The answer is the convexity of the partitions generated by centroids. Roughly,
if you draw a line between any given points in a partition then all the points
between those two points should also be in that partition. Thus the reason
why the partition found by $k$-means looks like the image on the left of
Figure \ref{fig:partition}.

\begin{figure}
\begin{center}
\includegraphics[width=.49\linewidth]{chapter_2/files/voronoi-cell.jpg}
\includegraphics[width=.49\linewidth]{chapter_2/files/intuition-cell.jpg}
\end{center}
\caption{Partitions induced by Voronoi cells of the centroids (left) and
partitions induced by our intuitive clustering (right)}
\label{fig:partition}
\end{figure}

The partitions induced by centroids are called Voronoi cells: the Voronoi
cell of a point $x$ of a metric space $\mathcal{X}$ is the set of points
$y$ whose distance to $x$ is not greater than any other points $x' \in \mathcal{X}$.
Each partition of $k$-means is a Voronoi cell of a centroid. It is a fact that
Voronoi cells are convex.

However, convexity is not always what we're looking for in a partition. Our intuition
roughly corresponds to the partition on the right of  Figure \eqref{fig:partition}.

If you draw a line between any two yellow points then that line will
cross the red section. This implies that the yellow partition is not a convex
set. Thus this partition is not achievable by $k$-means. As we will see
in the next section, density-based clustering can address this shortcoming.

The second drawback of centroid-based clustering methods is that we have
to specify the number of clusters we want. This can be quite difficult. Given
a dataset of more than 3 dimension, how do we even know what number of clusters
we want if we can't even visualize the data?

Density-based clustering is designed to address these issues.
The intuition behind this method is that "dense" regions of data
that are close together should be in the same cluster. To make an analogy,
the high-density regions, which are clusters in density-based clustering, are the continents,
while the low-density regions is the ocean that surround and separate these
continents.

Since high-density regions are not necessarily convex, we have escaped the curse of
convexity! Furthermore, since we are simply following the density of the data,
we do not have to specify the number of clusters before hand. We will see the
details in the next section.
